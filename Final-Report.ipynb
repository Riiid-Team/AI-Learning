{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riiid Project\n",
    "### Personalized Digital Education\n",
    "\n",
    "<strong>Darden Capstone Project\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a id='toc'></a>\n",
    "1. [Acquire](#acquire)\n",
    "2. [Prepare](#prepare)\n",
    "3. [Explore](#explore)\n",
    "4. [Modeling](#modeling)\n",
    "5. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Riiid Labs\n",
    "\n",
    "Riiid Labs, an AI solutions provider delivering creative disruption to the education market. With a strong belief in equal opportunity in education, Riiid launched an AI tutor based on deep-learning algorithms in 2017 that attracted more than one million South Korean students. This year, the company released EdNet, the worldâ€™s largest open database for AI education containing more than 100 million student interactions. [Source](https://www.kaggle.com/c/riiid-test-answer-prediction/overview/description)\n",
    "\n",
    "## Our Goal\n",
    "\n",
    "This project aims to create a machine learning model that can predict whether or not a user will answer a question correctly using data provided by Riiid Labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "# Train Validate Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Acquire and Prepare Files\n",
    "import acquire, prepare, explore, model\n",
    "from explore import rfe_ranker\n",
    "\n",
    "# Modeling Modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Acquire<a id='acquire'></a>\n",
    "Acquire the data used for this project. \n",
    "\n",
    "[Return to Top](#toc)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data from local files\n",
    "df_train = pd.read_csv('sampled_trainset.csv')\n",
    "df_validate = pd.read_csv('validate.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "df_train.shape, df_validate.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Data\n",
    "df_validate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Data\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire Takeaways\n",
    "- Data acquired from Kaggle: lectures.csv, questions.csv, and train.csv.\n",
    "- Created a function to create a merged csv file of train, lectures, and questions.\n",
    "- Converted the data types of numeric columns to reduce memory usage.\n",
    "- Created a function to select a random sample of 100K users.\n",
    "- Created a function to split the merged dataset into train, validate, and test.\n",
    "    - 80% of a users data in train, 10% in validate, and 10% in test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare<a id='prepare'></a>\n",
    "Prepare the data for exploration and created scaled features for modeling.<br>\n",
    "\n",
    "[Return to Top](#toc)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Prepare Function (found in prepare.py module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce train/validate/test and scaled train/validate/test dataframes\n",
    "train, validate, test, train_s, validate_s, test_s = prepare.prep_riiid(df_train, df_validate, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of train\n",
    "print(train.shape)\n",
    "\n",
    "# Display first 5 rows of train\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the shape of train scaled\n",
    "print(train_s.shape)\n",
    "\n",
    "# Display first 5 rows of train scaled \n",
    "train_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of validate \n",
    "print(validate.shape)\n",
    "\n",
    "# Display first 5 rows of validate\n",
    "validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of validate scaled \n",
    "print(validate_s.shape)\n",
    "\n",
    "# Display first 5 rows of validate scaled \n",
    "validate_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of test\n",
    "print(test.shape)\n",
    "\n",
    "# Display first 5 rows of test\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of test scaled\n",
    "print(test_s.shape)\n",
    "\n",
    "# Display first 5 rows of test scaled\n",
    "test_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Takeaways\n",
    "Used __prep_riiid__ function from __prepare.py__ to make the following changes:\n",
    "- Filled missing boolean values in question_had_explanation with False. Missing values indicated that the question did not have an explanation or the user viewed a lecture.\n",
    "- Filled missing values in prior_question_elapsed_time with 0. Missing values indicated that a user viewed a lecture before answering the first question in a bundle.\n",
    "- Dropped columns: lecture_id, tag, lecture_part, type_of, question_id, bundle_id, correct_answer, question_part, and tags\n",
    "- Dropped rows that represent lectures\n",
    "- Filled null and inf values with 0\n",
    "- Converted True and False values to 1's and 0's, respectively\n",
    "- Created scaled versions of the train, validate and test sets\n",
    "\n",
    "<b>Feature Engineering: (all in prepare.py file)</b> \n",
    "- Added column, user_lectures_running_total keeps a running total of the number of lectures the user has viewed\n",
    "- Added column, user_acc_mean, that reflects the avg accuracy of a user on all questions they answered\t\n",
    "- Added column, mean_container_task\n",
    "- Added column, mean_content_accuracy, that reflects average accuracy of all users answering a particular question\n",
    "- Added column, mean_task_accuracy, that reflects avg accuracy of all users answering a specific group of questions\n",
    "- Added column to train, q_time, that reflects the amount of milliseconds it took the user to answer the current question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3. Explore<a id='explore'></a>\n",
    "Created graphs, ran feature selection, and performed hypothesis testing to discover drivers of answering a question correctly.<br>\n",
    "\n",
    "[Return to Top](#toc)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and null value counts\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking # of unique values of columns with int64 dtypes.\n",
    "for col in train:\n",
    "    if train[col].dtypes == 'int64': \n",
    "        print(f'{col} has {train[col].nunique()} unique values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of features\n",
    "sns.set_context('talk')\n",
    "\n",
    "train.hist(figsize=(20, 20),\n",
    "           grid=True,\n",
    "           color='#0080ff',\n",
    "           ec='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting all features to understand how their values are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User_id vs Timestamp\n",
    "# Had to convert time from in milliseconds to weeks\n",
    "sns.set_context('talk')\n",
    "ts = df_train['timestamp']/(31536000000/52)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ts.plot.hist(bins=100, color='#0080ff', ec='black') \n",
    "plt.title(\"Most Users Leave After the First Week\", fontsize=20)\n",
    "plt.xlabel(\"Weeks Of Interaction\", fontsize=15)\n",
    "plt.ylabel(\"Number of Users (in Millions)\", fontsize=15)\n",
    "plt.xticks(np.linspace(0,100,11),rotation=0, fontsize=15)\n",
    "plt.yticks(np.arange(1_000_000,8_000_000, 1_000_000), labels=['1','2','3','4','5','6', '7'], rotation=0, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most users stop using the platform after the first week.\n",
    "- The rest of the users stop using the platform after ten weeks.\n",
    "- This shows us who our users are and platform usage behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Questions Answered Correct\n",
    "sns.set_context('talk')\n",
    "plt.figure(figsize=(10,6))\n",
    "train.answered_correctly.value_counts().sort_index().plot.bar(grid=True, color=['#d55e00', '#009e73'], ec='black')\n",
    "answer_correctly = train.answered_correctly.mean()\n",
    "plt.title(\"Number of Questions Answered Correctly\",fontsize=20) \n",
    "plt.xlabel('Answered Correctly',fontsize=15)\n",
    "plt.ylabel('Count (in Millions)',fontsize=15)\n",
    "plt.xticks([0,1],labels=['False','True'],rotation=0,fontsize=15) \n",
    "plt.yticks(np.arange(5_000_000,15_000_001, 5_000_000), labels=['5', '10', '15'],rotation=0,fontsize=15) \n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most questions are answered correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot \n",
    "# Plot displays how students perform on questions with and without explanations\n",
    "explore.question_explanation_graph(train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Users perform better on questions with explanations, than questions without explanations. \n",
    "    - A possible reason for this could be that the guidance from the explanations are actually helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of Questions Answered Correct vs. Number of Questions Answered Per User\n",
    "user_percent = df_train[df_train.answered_correctly != -1].groupby('user_id')['answered_correctly'].agg(Mean='mean', Answers='count')\n",
    "user_percent = user_percent.query('Answers <= 1000')\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "x = user_percent.Answers\n",
    "y = user_percent.Mean\n",
    "plt.scatter(x, y, marker='o',color='#0080ff')\n",
    "plt.title(\"Answering More Questions Leads to Slightly Higher Accuracy\", fontsize=20)\n",
    "plt.xlabel(\"Number of Questions Answered\", fontsize=15)\n",
    "plt.ylabel(\"Percent Answered Correct\", fontsize=15)\n",
    "plt.xticks(rotation=0, fontsize=15)\n",
    "plt.yticks(rotation=0, fontsize=15)\n",
    "\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x,p(x),\"r--\")\n",
    "plt.axhline()\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a slight positive linear trend for the relationship between percentage answered correct and the number of the questions answered. \n",
    "\n",
    "- Most users answer less than 200 question on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test 1\n",
    "\n",
    "### General Inquiry\n",
    "<br>\n",
    "Does getting a question right depend on it having an explanation?\n",
    "\n",
    "### Chi-Squared Test\n",
    "\n",
    "$H_0$: `question_had_explanation` and `answered_correctly` are independent\n",
    "<br>\n",
    "$H_a$: `question_had_explanation` and `answered_correctly` are dependant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating crosstab of data reflecting if question was answered correctly \n",
    "# and if it had an explanation.\n",
    "observed_pqhe = pd.crosstab(train.answered_correctly, \n",
    "                            train.question_had_explanation)\n",
    "\n",
    "# Displaying crosstab\n",
    "observed_pqhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating chi^2, p value, degrees of freedom and expected values\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed_pqhe)\n",
    "\n",
    "# Printing results\n",
    "print('alpha = .05\\n')\n",
    "print(f'p = {p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P is less than alpha, so we <b>reject</b> the null hypothesis that answered_correctly and question_had_explanation are independent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test 2\n",
    "\n",
    "### General Inquiry\n",
    "Does getting a question right depend on the part of the exam?\n",
    "\n",
    "### Chi-Squared Test\n",
    "$H_0$ Whether a user answers a question correctly is independent of the type of question being asked.<br>\n",
    "$H_a$ Whether a user answers a question correctly is dependent upon the type of question being asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table\n",
    "table = pd.crosstab(train.answered_correctly, train.part)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(table)\n",
    "\n",
    "# Printing results\n",
    "print('alpha = .05\\n')\n",
    "print(f'p = {p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P is less than alpha, so we <strong>reject</strong> the null hypothesis that answering a question correctly is independent of the type of question being asked. (Different parts of the TOEIC exam)\n",
    "- The 7 parts of the TOEIC exam require the user to answer questions with different formats: Pictures, Listening to Conversations, Reading Conversations, Filling in Incomplete Sentences, etc.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test 3\n",
    "\n",
    "### General Inquiry\n",
    "<br>\n",
    "Is there a linear relationship between the average time it takes a user to answer a question and their average accuracy?\n",
    "\n",
    "### Pearson Correlation Test\n",
    "\n",
    "$H_0$: There is no linear relationship between `avg_user_q_time` and `user_acc_mean`\n",
    "<br>\n",
    "$H_a$: There is a linear relationship between `avg_user_q_time` and `user_acc_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing pearson correlation test\n",
    "r, p = stats.pearsonr(train.avg_user_q_time, train.user_acc_mean)\n",
    "\n",
    "# Displaying alpha and test results\n",
    "print(\"alpha = .05\\n\")\n",
    "print(\"r correlation coefficient\", np.round(r, 4))\n",
    "print(\"p value                   \", np.round(p, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P is less than alpha, so we <b>reject</b> the null hypothesis that there is no linear relationship between the variables.\n",
    "- Given the r correlation coefficient of -.08, we see almost no relationship between the variables.\n",
    "- Although the relationship between these variables is weak, it was still worth exploring because to needed to see the correlation between these variables.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test 4\n",
    "\n",
    "### General Inquiry\n",
    "<br>\n",
    "Do users with high accuracy take less time to answer difficult questions than other users?\n",
    "\n",
    "### Two-Sample Two-Tailed T-Test\n",
    "$H_0$: The avg time that users with above avg accuracy spend on questions with below avg content accuracy is equal to or greater than the avg time that users with avg or below accuracy spend on them.\n",
    "<br><br>\n",
    "$H_a$: The avg time that users with above avg accuracy spend on questions with below avg content accuracy is less than the avg time that users with avg or below accuracy spend on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating df of all questions that had less than average mean_content_accuracy\n",
    "# ie. all questions that users answered incorrectly more often than average\n",
    "hard_questions = train[train.mean_content_accuracy < train.mean_content_accuracy.mean()]\n",
    "\n",
    "# Filtering DF above for all users with above avg accuracy\n",
    "hi_acc_users = hard_questions[hard_questions.user_acc_mean > hard_questions.user_acc_mean.mean()]\n",
    "\n",
    "# Filtering DF above for all users with avg or below accuracy\n",
    "low_acc_users = hard_questions[hard_questions.user_acc_mean <= hard_questions.user_acc_mean.mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing two sample, two tailed t-test\n",
    "t, p = stats.ttest_ind(hi_acc_users.q_time, low_acc_users.q_time, equal_var = False)\n",
    "\n",
    "# Printing results\n",
    "print('alpha = .05\\n')\n",
    "print(\"test statistic   \", np.round(t, 4))\n",
    "print(\"p value           \", np.round(p, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P is less than alpha, so we <b>reject</b> the null hypothesis. \n",
    "- The sub-alpha p-value and negative test statistic indicate that users with higher accuracy spend less time on difficult questions than others. \n",
    "    - The explanation for this might be that many high accuracy users are well prepared for difficult questions than users with lower accuracy. and not that they are spending more time on hard questions to improve their performance.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test 5\n",
    "\n",
    "### General Hypothesis\n",
    "<br>\n",
    "Is there a linear relationship between the number of lectures a user has viewed and their task performance? Task performance measures a user's accuracy on a specific bundle of questions. A bundle can contain one or more questions.\n",
    "\n",
    "### Pearson Correlation Test\n",
    "\n",
    "$H$0: There is no linear relationship between `user_lectures_running_total` and `mean_task_accuracy`\n",
    "<br>\n",
    "$H$a: There is a linear relationship between `user_lectures_running_total` and `mean_task_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing pearson correlation test\n",
    "r, p = stats.pearsonr(train.user_lectures_running_total, train.mean_task_accuracy)\n",
    "\n",
    "# printing alpha and results\n",
    "print(\"alpha = .05\\n\")\n",
    "print(\"r correlation coefficient\", np.round(r, 2))\n",
    "print(\"p value                  \", np.round(p, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since p is less than alpha, we <b>reject</b> the null hypothesis that there is no linear relationship between `mean_task_accuracy` and `user_lectures_running_total`.    \n",
    "\n",
    "- The r correlation coefficient of .37 indicates that there is a weak linear relationship between these variables.\n",
    "    - As the number of lectures that a user has seen increases, so does their task accuracy on average.\n",
    "    - Viewing lectures has a positive, albeit weak, impact on user performance.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "### Separate the features from the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, validate, and test into X_set and y_sets\n",
    "X_train = train_s.drop(columns='answered_correctly')\n",
    "y_train = train_s['answered_correctly']\n",
    "\n",
    "X_validate = validate_s.drop(columns='answered_correctly')\n",
    "y_validate = validate_s['answered_correctly']\n",
    "\n",
    "X_test = test_s.drop(columns='answered_correctly')\n",
    "y_test = test_s['answered_correctly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 rows of X_train\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 values y_train\n",
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Top 5 Features by Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the top 5 features using SelectKBest\n",
    "f_features = explore.KBest_ranker(X_train, y_train, 5)\n",
    "\n",
    "# Display the top 5 features\n",
    "f_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Top 5 Features by Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def rfe_ranker(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Accepts dataframe. Uses Recursive Feature Elimination to rank the given df's features in order of their usefulness in\n",
    "    predicting logerror with a logistic regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Logistic Regression object\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    # Fit the object with the training set\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Create a Recursive Feature Elimination object. Rank 1 feature as the most important.\n",
    "    rfe = RFE(lr, 1)\n",
    "\n",
    "    # Fit the RFE object with the training data to \n",
    "    rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    # creating mask of selected feature\n",
    "    feature_mask = rfe.support_\n",
    "\n",
    "    # creating ranked list \n",
    "    feature_ranks = rfe.ranking_\n",
    "\n",
    "    # creating list of feature names\n",
    "    feature_names = X_train.columns.tolist()\n",
    "\n",
    "    # create df that contains all features and their ranks\n",
    "    rfe_ranks_df = pd.DataFrame({'feature': feature_names,\n",
    "                                 'rank': feature_ranks})\n",
    "\n",
    "    # return df sorted by rank\n",
    "    rfe_ranked_featuers = rfe_ranks_df.sort_values('rank').reset_index(drop=True)\n",
    "    \n",
    "    return rfe_ranked_featuers.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using function from explore.py to rank using RFE\n",
    "rfe_ranker(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Takeaways\n",
    "### Graphs:\n",
    "- Most users leave after the first week.\n",
    "- Of all questions asked, users tend to answer most of them correctly.\n",
    "- Questions that have explanations are more likely to be answered correctly.\n",
    "- Answering more questions leads to slightly higher accuracy.\n",
    "    \n",
    "### Hypothesis Testing:\n",
    "|  | Statistical Test | Result |\n",
    "| :----  | :--------------- | :-------|\n",
    "| <b>Hypothesis 1</b> |  Chi-squared | A question with an explanation and the question being answered correctly is dependent. |\n",
    "| <b>Hypothesis 2</b> | Chi-squared | Getting a question right is dependent on the part of the exam. |\n",
    "| <b>Hypothesis 3</b> | Pearson's Correlation |  An inverse linear relationship exists between the average time users spend on a question and their average accuracy.  |\n",
    "| <b>Hypothesis 4</b> | Two-sample One-tailed T-Test | High accuracy users spend less time on hard questions than other users with lower accuracy. |\n",
    "| <b>Hypothesis 5</b> | Pearson's Correlation | A linear relationship exists between average task accuracy and the number of lectures a user has viewed.|\n",
    "\n",
    "### Feature Selection:\n",
    "The top 4 features shared between __Select K Best__ and __RFE__ rankers were:\n",
    "- `mean_content_accuracy`\n",
    "- `user_acc_mean`\n",
    "- `mean_task_accuracy`\n",
    "- `prior_question_had_explanation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling<a id='modeling'></a>\n",
    "Establish a baseline and create several classification models that predict whether a student answers a question correctly.<br>\n",
    "\n",
    "[Return to Top](#toc)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the baseline AUC score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the randomly generated classes as the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the size of y_train\n",
    "size = y_train.size\n",
    "\n",
    "# Construct y_baseline\n",
    "y_baseline_random = np.random.randint(2, size=size)\n",
    "\n",
    "# Print the y_baseline\n",
    "y_baseline_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC score \n",
    "roc_auc_score(y_train, y_baseline_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the most frequent class as the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the most frequent class\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the y_baseline_frequency\n",
    "y_baseline_frequency = np.array([1]*size)\n",
    "\n",
    "# Check the baseline values\n",
    "y_baseline_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC score \n",
    "roc_auc_score(y_train, y_baseline_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Results**<br>\n",
    "The baseline AUC score is 0.5, regardless of the method used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classification Models\n",
    "\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_auc_score\n",
    "\n",
    "def auc_score_proba(clf, X, y):\n",
    "    '''\n",
    "    This function accepts a classification model that can estimate probability, X_set and y_set\n",
    "    and returns a dataframe of predicted probabilty on a binary class\n",
    "    and also returns a auc score\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    clf: the classification algorithm after fitting on X_train, y_train\n",
    "    X: X_train, X_validate and X_test\n",
    "    y: y_train, y_validate and y_test\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    1. A dataframe containing the probability estimates\n",
    "    2. AUC score\n",
    "    '''\n",
    "    y_proba = clf.predict_proba(X)\n",
    "    y_proba = pd.DataFrame(y_proba, columns=['p_0', 'p_1'])\n",
    "    score = roc_auc_score(y, y_proba['p_1'])\n",
    "    return y_proba, score\n",
    "\n",
    "def model_multiple_algos(names, classifiers, X_train, y_train):\n",
    "    '''\n",
    "    This function accetps a list of classifiers, feature dataset and target dataset\n",
    "    and return the auc scores.\n",
    "    The order of the names should match the order of the classifiers\n",
    "    Parameter\n",
    "    ----------\n",
    "    names: a list of the names of the classifiers that will be tested.\n",
    "    classifiers: a list of classifier objects.\n",
    "    X_train: features in the train dataset\n",
    "    y_train: target variable in the train dataset\n",
    "    X_validate: features in the validate dataset\n",
    "    y_validate: target variable in the validate dataset\n",
    "    X_test: features in the test dataset\n",
    "    y_test: target variable in the test dataset\n",
    "    Example\n",
    "    ----------\n",
    "    names: [\"logistic Regression\", \"Decision Tree\"]\n",
    "    classifiers: [LogisticRegression(), DecisionTreeClassifier(max_depth=3)]\n",
    "    all the datasets ready for modeling\n",
    "    Return\n",
    "    ----------\n",
    "    A dataframe of auc scores associated with the classification algorithm and the dataset it used. \n",
    "    '''\n",
    "    \n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        # Set up a progress indicator        \n",
    "        print(f\"{name}\") \n",
    "        \n",
    "        # Fit on the train dataset        \n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute the AUC score on train\n",
    "        y_proba, score = auc_score_proba(clf, X_train, y_train)\n",
    "        d = {\"Algo\": name,\n",
    "             \"dataset\": \"train\",\n",
    "             \"AUC score\": score}\n",
    "        \n",
    "        metrics = metrics.append(d, ignore_index=True)\n",
    "        \n",
    "        # Show the completeness of the modeling\n",
    "        print(f\"{name} has completed\")\n",
    "             \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of names of the classifiers\n",
    "# Make sure all the classifiers have the method: predict_proba\n",
    "names = ['LogisticRegression',\n",
    "         'Random Forest',\n",
    "         'Gradient Boost',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers\n",
    "classifiers = [LogisticRegression(), \n",
    "               RandomForestClassifier(max_depth=3),\n",
    "               GradientBoostingClassifier(),\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check: the size between the two should be the same\n",
    "len(names) == len(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the classifiers created above to model the train, validate and test\n",
    "metrics = model_multiple_algos(names, classifiers, \n",
    "                                     X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank the auc scores on test dataset in the descending order\n",
    "mask = (metrics.dataset == 'test')\n",
    "metrics[mask].sort_values(by=\"AUC score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling Results**\n",
    "- Random Forest produces the highest AUC score on the test dataset.\n",
    "- The AUC score from the top model is 0.692.\n",
    "- The AUC score for the baseline is 0.500.\n",
    "- The top model surpassed the baseline by 0.192, a 38.4% improvement\n",
    "     - Calculated using difference between the our model's AUC score minus the baseline AUC score, divided by the baseline AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ROC curve on the top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lr object and fit on train\n",
    "lr = RandomForestClassifier(max_depth=3)\n",
    "lr = lr.fit(X_train, y_train)\n",
    "\n",
    "# Plot the ROC curve on test\n",
    "model.auc_curve_plot1(lr, \"Random Forest\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Takeaways\n",
    "- <b>Baseline:</b> 0.5 AUC Score  \n",
    "\n",
    "\n",
    "- <b>Top 3 performing models: (results are from the test set)</b> \n",
    "    - Random Forest: 0.692   \n",
    "    - Logistic Regression: 0.691\n",
    "    - Neural Net: 0.687    \n",
    "    \n",
    "    \n",
    "\n",
    "- <b>Features used on all models:</b>\n",
    "    - mean_content_accuracy\n",
    "    - user_acc_mean\n",
    "    - mean_task_accuracy\n",
    "    - question_had_explanation\n",
    "    - user_lectures_running_total\n",
    "    - avg_user_q_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions<a id='conclusions'></a>\n",
    "## Summary of key takeaways, results, and next steps<br>\n",
    "\n",
    "[Return to Top](#toc)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire\n",
    "- Data initially acquired from Kaggle and saved as local files: lectures.csv, questions.csv, and train.csv.\n",
    "- Merged the csv files into a single dataset.\n",
    "\n",
    "### Prepare\n",
    "- Filled missing values with appropriate values varying according to the column.\n",
    "- Dropped columns that were not needed. \n",
    "- Dropped rows that represent lectures.\n",
    "- Converted True and False values to 1's and 0's, respectively.\n",
    "- Created scaled versions of the train, validate, and test sets.\n",
    "- Added engineered features to the train, validate, and test sets.\n",
    "\n",
    "### Explore\n",
    "\n",
    "__Graphs__\n",
    "- Most users leave after the first week.\n",
    "<br>\n",
    "- The majority of the questions asked are answered right.\n",
    "<br>\n",
    "- Questions that have explanations are more likely to be answered correctly.\n",
    "<br>\n",
    "- Answering more questions leads to slighty higher accuracy for students.\n",
    "\n",
    "__Hypothesis Testing__\n",
    "\n",
    "- Hypothesis 1: Chi^2 test results show that a question having an explanation and the question being answered correctly are dependant.\n",
    "\n",
    "- Hypothesis 2: Chi^2 test results show that getting a question right is dependent on the part of the exam.\n",
    "\n",
    "- Hypothesis 3: Pearson correlation test results reflect that there is an extremely weak inverse linear relationship between the average time a user spends on question and their average accuracy. \n",
    "\n",
    "- Hypothesis 4: Two-sample one-tailed t-test results suggest that high accuracy users \n",
    "\n",
    "- Hypothesis 5: Pearson correlation test results reflect that there is a weak linear relationship between average task accuracy and the amount of lectures a user has viewed. \n",
    "\n",
    "__Feature Selection__\n",
    "The top 4 features that Select K Best and RFE rankers found in common were:\n",
    "- mean_content_accuracy\n",
    "- user_acc_mean\n",
    "- mean_task_accuracy\n",
    "- prior_question_had_explanation (renamed to 'question_had_explanation')\n",
    "\n",
    "### Modeling\n",
    "\n",
    "__Overall__ \n",
    "- The highest AUC score on the test dataset is produced by Random Forest.\n",
    "- The top model surpassed the baseline by 0.192, which is a 38.4% improvement (which is a comparison of the difference between the scores divided by the baseline).\n",
    "\n",
    "__Recommendations__\n",
    "- Riiid should provide more questions with explanations for students. \n",
    "- Focus on improving user engagement after the first week to increase user retention. \n",
    "- Using our project, Riiid can assess: \n",
    "    - ... question difficulty level based on population performance (mean_content_accuracy and mean_task_accuracy).\n",
    "    - ... an individual's performance based on their average performance (user_acc_mean). \n",
    "    - ... how time spent on a question impacts a student's accuracy (avg_user_q_time_scaled). \n",
    "\n",
    "- Using our model, Riiid can predict the future performance of current students. \n",
    "\n",
    "__Expectations__\n",
    "- Providing more explanations could improve overall answer accuracy. \n",
    "- Increasing user retention would give a better representation of student long-term performance. \n",
    "\n",
    "\n",
    "### Future Investigations\n",
    "- Use this predictive model on Riiid's other educational programs. \n",
    "- Explore more features and different classification models like xgboost and lightlgbm.\n",
    "- Improve model by to predict new student performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
