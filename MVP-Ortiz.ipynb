{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import acquire\n",
    "\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in primary dataset.\n",
    "df = pd.read_csv('train.csv', usecols=[1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training dataset\n",
    "To reduce the memory size of our training data, we selected a random sample of 100,000 students from `train.csv`.\n",
    "1. Select 100,000 user ids from `train.csv` to use as our train dataset.\n",
    "2. Cast numeric column data types using an appropriate precision to reduce memory size.\n",
    "\n",
    "## Sample training data\n",
    "- Selecting 100_000 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all users with more than 10 rows.\n",
    "user_ids = df['user_id'].value_counts()[df['user_id'].value_counts() > 10].index.to_list()\n",
    "\n",
    "# Select a random sample of 100_000 user_ids.\n",
    "sampled_ids = random.sample(user_ids, 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled_users(df):\n",
    "    '''\n",
    "    This function accepts data from `train.csv` and\n",
    "    returns a random sample of 100_000 user_ids.\n",
    "    '''\n",
    "    user_ids = df['user_id'].value_counts()[df['user_id'].value_counts() > 10].index.to_list()\n",
    "    sampled_ids = random.sample(user_ids, 100_000)\n",
    "    return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the selected user_ids, filter the dataset for the first 100_000.\n",
    "filtered_data = pd.DataFrame()\n",
    "\n",
    "data = df.loc[df['user_id'].isin(sampled_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 100_000 users!!!!\n",
    "data.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache local file for next steps.\n",
    "# data.to_csv('sampled_train.csv', index=False)\n",
    "\n",
    "# Dataset loads correctly.\n",
    "# df = pd.read_csv('sampled_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datatype_converter():\n",
    "    '''\n",
    "    This function returns a dictionary of column names and data types to convert.\n",
    "    '''\n",
    "    \n",
    "    train_data_types_dict = {\n",
    "    'timestamp': np.int64,\n",
    "    'user_id': np.int32,\n",
    "    'content_id': np.int16,\n",
    "    'content_type_id': np.int16,\n",
    "    'task_container_id' : np.int16,\n",
    "    'user_answer' : np.int8,\n",
    "    'answered_correctly': np.int8,\n",
    "    'prior_question_elapsed_time': np.float16\n",
    "    }\n",
    "    \n",
    "    lectures_data_types_dict = {\n",
    "    'lecture_id' : np.int16,\n",
    "    'tag' : np.int8,\n",
    "    'part' : np.int8\n",
    "    }\n",
    "\n",
    "    questions_data_types_dict = {\n",
    "    'question_id' : np.int16,\n",
    "    'bundle_id' : np.int16,\n",
    "    'part' : np.int8\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return train_data_types_dict, lectures_data_types_dict, questions_data_types_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled_train():\n",
    "    '''\n",
    "    This function selects a random sample of 100_000 users from the `train.csv` dataset.\n",
    "    Returns a dataframe of 100_000 users that have more than 10 rows of data.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : pandas.core.frame.DataFrame\n",
    "        A pandas dataframe of 100,000 randomly selected\n",
    "        users.\n",
    "    '''\n",
    "    train_dtypes, _, _ = datatype_converter()\n",
    "    \n",
    "    if os.path.isfile('sampled_train.csv'):\n",
    "        return pd.read_csv('sampled_train.csv',\n",
    "                           index_col=False,\n",
    "                           dtype=train_dtypes)\n",
    "    else:\n",
    "        \n",
    "    # Read in `train.csv` data\n",
    "        df = pd.read_csv('train.csv', dtype=train_dtypes, usecols=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "        sampled_ids = sampled_users(df)\n",
    "\n",
    "        sampled_data = df.loc[df['user_id'].isin(sampled_ids)]\n",
    "\n",
    "        # Cache local file of sampled data.\n",
    "        sampled_data.to_csv('sampled_train.csv', index=False)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to reproduce the modified training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function.\n",
    "df = sampled_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works.\n",
    "df.user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our data types and memory usage.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, lecture_dtypes, question_dtypes = datatype_converter()\n",
    "\n",
    "df_lectures = pd.read_csv('lectures.csv', dtype=lecture_dtypes)\n",
    "df_questions = pd.read_csv('questions.csv', dtype=question_dtypes)\n",
    "\n",
    "# Left join df_train and df_lectures using `content_id` as the primary key.\n",
    "df_merged = df.merge(df_lectures, left_on='content_id', right_on='lecture_id', how='left')\n",
    "\n",
    "# Left join df_merged and df_questions using `content_id` as the primary key.\n",
    "df_data = df_merged.merge(df_questions, left_on='content_id', right_on='question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_riiid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "validate = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "# Set up the train size\n",
    "train_size = 0.8\n",
    "validate_size = 0.1\n",
    "\n",
    "sampled_ids = df.user_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in sampled_ids:\n",
    "    data = df.loc[df['user_id'] == user]\n",
    "    n = data.shape[0]\n",
    "\n",
    "    train_end_index = int(train_size * n)\n",
    "    validate_end_index = train_end_index + int(validate_size * n)\n",
    "\n",
    "    df_train = data.iloc[:train_end_index]\n",
    "    df_validate = data.iloc[train_end_index:validate_end_index]\n",
    "    df_test = data.iloc[validate_end_index:]\n",
    "\n",
    "    train = pd.concat([train, df_train])\n",
    "    validate = pd.concat([validate, df_validate])\n",
    "    test = pd.concat([test, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the shape of the original, train and test\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('sampled_trainset.csv', index=False)\n",
    "validate.to_csv('validate.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function to split data using Quasi-GroupKFold method\n",
    "The way our data is currently split for MVP:\n",
    "- 100,000 randomly selected users that have more that 10 interactions with Riiid's Knowledge tracing application.\n",
    "- Each user has _sequential_ data, indicated by the variable `timestamp`.\n",
    "- Data is split using a percentage-based method.\n",
    "- 0% - 80% of a users data is the training set.\n",
    "- 80% - 90% of a users data is in the validation set.\n",
    "- 90% - 100% of a users data is in the test set.\n",
    "\n",
    "The way we split the data is important. As we currently have our splits, several issues arise that impact our data exploration and modeling performance.\n",
    "- Spliting the data using a percentage-based method removes questions and lectures from our training data. If a model encounters a question it has never seen before, how can it accurately model reality? We need to have all questions appear at least once in our dataset.\n",
    "    - If this was _purely_ a time series problem, that would be fine.\n",
    "- The training set uses 80% of a users data, this impacts our statistical analysis. If we have _all_ of a users data, we can correctly calculate population statistics from a _sample_ of users. We can then compare statistics on a user/grouped level with the population.\n",
    "\n",
    "What is the solution?\n",
    "\n",
    "> <strong>Splitting by users!</strong>\n",
    "\n",
    "## Splitting by users\n",
    "How does this solve our exploration and modeling issues?\n",
    "> <strong>By creating users the model has never seen before!</strong>\n",
    "\n",
    "This simulates _new_ users interacting with Riiid's Knowledge Tracing Application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = acquire.get_riiid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.lecture_id = df_data.lecture_id.astype('Int16')\n",
    "df_data.tag = df_data.tag.astype('Int8')\n",
    "df_data.part_x = df_data.part_x.astype('Int8')\n",
    "df_data.part_y = df_data.part_y.astype('Int8')\n",
    "df_data.question_id = df_data.question_id.astype('Int16')\n",
    "df_data.bundle_id = df_data.bundle_id.astype('Int16')\n",
    "df_data.lecture_id = df_data.lecture_id.astype('Int32')\n",
    "\n",
    "# Prefix part names with the originating dataframe name.\n",
    "df_data.rename(columns={'part_x': 'lecture_part',\n",
    "                        'part_y': 'question_part'},\n",
    "                   inplace=True)\n",
    "\n",
    "# Cache the data.\n",
    "df_data.to_csv('riiid_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many users are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df_data.user_id.unique()\n",
    "print(f'There are {len(users):,} users.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new data splits using percentage of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages of orginal dataset\n",
    "train_size = .8\n",
    "validate_size = .1\n",
    "test_size = .1\n",
    "\n",
    "# Calculate the number of users in each dataset.\n",
    "train_users = int(len(users)*train_size)\n",
    "validate_users = math.ceil(len(users)*validate_size)\n",
    "test_users = math.ceil(len(users)*test_size)\n",
    "\n",
    "# Display the results.\n",
    "print(f'Train set would contain {train_users:,} users')\n",
    "print(f'Validate set would contain {validate_users:,} users')\n",
    "print(f'Test set would contain {test_users:,} users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for even splits\n",
    "print(len(users) == sum([train_users, validate_users, test_users]))\n",
    "\n",
    "print(len(users))\n",
    "print(sum([train_users, validate_users, test_users]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a method to split users into seperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed to reproduce splits\n",
    "random.seed(123)\n",
    "\n",
    "# Toy example\n",
    "all_users = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Randomly select train set users\n",
    "t_ids = random.sample(all_users, 3)\n",
    "\n",
    "# Remove users assigned to the training set.\n",
    "list(set(all_users) - set(t_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to reproduce data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_users(df, train_size=.8, validate_size=.1, test_size=.1):\n",
    "    '''\n",
    "    This function accepts the merged dataframe from acquire.get_riiid_data()\n",
    "    and returns train, validate and test sets.\n",
    "    '''\n",
    "    # Set a random seed to reproduce splits\n",
    "    random.seed(123)\n",
    "    \n",
    "    # Gather all user ids\n",
    "    user_ids = list(df['user_id'].unique())\n",
    "    total_num = len(user_ids)\n",
    "    \n",
    "    # Calculate the number of users in train, validate, and test.\n",
    "    train_num = int(total_num*train_size)\n",
    "    validate_num = math.ceil(total_num*validate_size)\n",
    "    test_num = math.ceil(total_num*test_size)\n",
    "    \n",
    "    # Randomly select 80% of the users to be in train.\n",
    "    train_ids = random.sample(user_ids, train_num)\n",
    "    \n",
    "    # Remove user_ids assigned to the training set.\n",
    "    remaining_val_test_users = list(set(user_ids) - set(train_ids))\n",
    "    \n",
    "    # Assign the remaining user ids to validate and test.\n",
    "    validate_ids = random.sample(remaining_val_test_users, validate_num)\n",
    "    test_ids = list(set(remaining_val_test_users) - set(validate_ids))\n",
    "    \n",
    "    # Return the users assigned to train, validate, and test\n",
    "    return train_ids, validate_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ids, validate_ids, test_ids = split_users(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_data.loc[df_data['user_id'].isin(train_ids)]\n",
    "validate = df_data.loc[df_data['user_id'].isin(validate_ids)]\n",
    "test = df_data.loc[df_data['user_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_k.csv', index=False)\n",
    "validate.to_csv('validate_k.csv', index=False)\n",
    "test.to_csv('test_k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the function to use a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_users(df, train_size=.8, validate_size=.1, test_size=.1, sample=True):\n",
    "    '''\n",
    "    This function accepts the merged dataframe from acquire.get_riiid_data()\n",
    "    and returns train, validate and test sets.\n",
    "    '''\n",
    "    # Set a random seed to reproduce splits\n",
    "    random.seed(123)\n",
    "    \n",
    "    if sample == True:\n",
    "        # Gather a random sample of 100_000 user ids\n",
    "        user_ids = random.sample(list(df['user_id'].unique()), 100_000)\n",
    "    else:\n",
    "        # Gather all user ids\n",
    "        user_ids = list(df['user_id'].unique())\n",
    "    \n",
    "    # Calculate the number of users\n",
    "    total_num = len(user_ids)\n",
    "    \n",
    "    # Calculate the number of users in train, validate. Remaining users go in test\n",
    "    train_num = int(total_num*train_size)\n",
    "    validate_num = math.ceil(total_num*validate_size)\n",
    "    \n",
    "    # Randomly select 80% of the users to be in train.\n",
    "    train_ids = random.sample(user_ids, train_num)\n",
    "    \n",
    "    # Remove user_ids assigned to the training set.\n",
    "    remaining_val_test_users = list(set(user_ids) - set(train_ids))\n",
    "    \n",
    "    # Assign the remaining user ids to validate and test.\n",
    "    validate_ids = random.sample(remaining_val_test_users, validate_num)\n",
    "    test_ids = list(set(remaining_val_test_users) - set(validate_ids))\n",
    "    \n",
    "    # Return the users assigned to train, validate, and test\n",
    "    return train_ids, validate_ids, test_ids\n",
    "\n",
    "\n",
    "def train_validate_test(df, sampled=True):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    train_ids, validate_ids, test_ids = split_users(df, sample=sampled)\n",
    "\n",
    "    train = df.loc[df['user_id'].isin(train_ids)]\n",
    "    validate = df.loc[df['user_id'].isin(validate_ids)]\n",
    "    test = df.loc[df['user_id'].isin(test_ids)]\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = train_validate_test(acquire.get_riiid_data())\n",
    "train.to_csv('mvp_train.csv', index=False)\n",
    "validate.to_csv('mvp_validate.csv', index=False)\n",
    "test.to_csv('mvp_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20478221 entries, 96 to 101228712\n",
      "Data columns (total 19 columns):\n",
      " #   Column                          Dtype  \n",
      "---  ------                          -----  \n",
      " 0   row_id                          int64  \n",
      " 1   timestamp                       int64  \n",
      " 2   user_id                         int32  \n",
      " 3   content_id                      int16  \n",
      " 4   content_type_id                 int16  \n",
      " 5   task_container_id               int16  \n",
      " 6   user_answer                     int8   \n",
      " 7   answered_correctly              int8   \n",
      " 8   prior_question_elapsed_time     float16\n",
      " 9   prior_question_had_explanation  object \n",
      " 10  lecture_id                      Int32  \n",
      " 11  tag                             Int8   \n",
      " 12  lecture_part                    Int8   \n",
      " 13  type_of                         object \n",
      " 14  question_id                     Int16  \n",
      " 15  bundle_id                       Int16  \n",
      " 16  correct_answer                  float64\n",
      " 17  question_part                   Int8   \n",
      " 18  tags                            object \n",
      "dtypes: Int16(2), Int32(1), Int8(3), float16(1), float64(1), int16(3), int32(1), int64(2), int8(2), object(3)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the function to use a subset of the data: Sample of 50K users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_users(df, train_size=.8, validate_size=.1, sample=True):\n",
    "    '''\n",
    "    This function accepts the merged dataframe from acquire.get_riiid_data()\n",
    "    and returns train, validate and test sets.\n",
    "    '''\n",
    "    # Set a random seed to reproduce splits\n",
    "    random.seed(123)\n",
    "    \n",
    "    if sample == True:\n",
    "        # Gather a random sample of 50_000 user ids\n",
    "        user_ids = random.sample(list(df['user_id'].unique()), 50_000)\n",
    "    else:\n",
    "        # Gather all user ids\n",
    "        user_ids = list(df['user_id'].unique())\n",
    "    \n",
    "    # Calculate the number of users\n",
    "    total_num = len(user_ids)\n",
    "    \n",
    "    # Calculate the number of users in train, validate. Remaining users go in test\n",
    "    train_num = int(total_num*train_size)\n",
    "    validate_num = math.ceil(total_num*validate_size)\n",
    "    \n",
    "    # Randomly select 80% of the users to be in train.\n",
    "    train_ids = random.sample(user_ids, train_num)\n",
    "    \n",
    "    # Remove user_ids assigned to the training set.\n",
    "    remaining_val_test_users = list(set(user_ids) - set(train_ids))\n",
    "    \n",
    "    # Assign the remaining user ids to validate and test.\n",
    "    validate_ids = random.sample(remaining_val_test_users, validate_num)\n",
    "    test_ids = list(set(remaining_val_test_users) - set(validate_ids))\n",
    "    \n",
    "    # Return the users assigned to train, validate, and test\n",
    "    return train_ids, validate_ids, test_ids\n",
    "\n",
    "\n",
    "def train_validate_test(df, sampled=True):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    train_ids, validate_ids, test_ids = split_users(df, sample=sampled)\n",
    "\n",
    "    train = df.loc[df['user_id'].isin(train_ids)]\n",
    "    validate = df.loc[df['user_id'].isin(validate_ids)]\n",
    "    test = df.loc[df['user_id'].isin(test_ids)]\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = train_validate_test(acquire.get_riiid_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('mvp_50_train.csv', index=False)\n",
    "validate.to_csv('mvp_50_validate.csv', index=False)\n",
    "test.to_csv('mvp_50_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
